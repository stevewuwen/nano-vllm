## 学习路线：从 PyTorch 基础到 vLLM 核心

### 阶段一：PyTorch 核心组件复习 (与 nanovllm/layers 强相关)

这部分对你来说应该是轻车熟路，目标是快速回顾在实现一个 Transformer 模型时最常用到的 PyTorch 组件。

1. `nn.Module`:
    * 复习点: 如何定义一个网络层（__init__中定义子模块）和前向传播逻辑（forward方法）。这是所有模型和层的基础。
    * 对应代码: nanovllm/layers 目录下的所有文件，例如 attention.py 中的 Attention 类，都是 nn.Module 的子类。
2. 核心网络层:
    * `nn.Linear`: 全连接层，Transformer中的Q,K,V投影、FFN层等的核心。
    * `nn.Embedding`: 将输入的 token ID 转换为向量。
    * `nn.LayerNorm`: Transformer Block 中的关键组件，用于稳定训练和推理。
    * 对应代码:
        * nanovllm/layers/linear.py (对 nn.Linear 的封装)
        * nanovllm/layers/embed_head.py (包含 nn.Embedding 和输出层的逻辑)
        * nanovllm/layers/layernorm.py (RMSNorm 的实现，一种 LayerNorm 的变体)
3. 张量操作 (Tensor Manipulation):
    * 复习点: view, reshape, transpose, permute 对张量维度的操作；cat, stack 对张量的拼接；bmm (批量矩阵乘法)；softmax。这些是实现 Attention 的基础。
    * 对应代码: nanovllm/layers/attention.py 中的 forward 方法里有大量的张量操作，是复习的最佳材料。

### 阶段二：Hugging Face Transformers & 模型加载 (与 nanovllm/utils 强相关)

nano-vllm 依赖 Hugging Face 生态来加载预训练模型权重和配置，理解这部分是打通代码流程的关键。

1. `transformers` 库的核心类:
    * `AutoConfig`: 从模型名称或路径加载 config.json，其中包含了模型的超参数（如层数、头数、隐藏层大小等）。
    * `AutoTokenizer`: 加载模型对应的 Tokenizer，用于文本的编码和解码。
    * `AutoModelForCausalLM`: 加载完整的预训练模型。nano-vllm 并没有直接用它，而是借鉴了它的权重加载逻辑。
    * 复习点: 理解 from_pretrained("model-name") 这个方法背后做了什么：下载/查找模型文件 -> 根据 config.json 构建模型结构 -> 加载权重文件 (.safetensors 或 
      .bin)。

2. 模型文件:
    * `config.json`: 模型的架构定义文件。
    * `model.safetensors`: 新一代安全的模型权重存储格式。
    * 复习点: 知道 nano-vllm 是如何读取这些文件，并将权重加载到自己定义的模型结构中的。
    * 对应代码: nanovllm/utils/loader.py 中的 load_hf_weights 函数是核心。它展示了如何解析 Hugging Face 的权重并加载到 nano-vllm 的模型中。

### 阶段三：vLLM 核心概念与 PagedAttention (与 nanovllm/engine 强相关)

这是你的最终目标。在复习完前两个阶段后，你就可以全力攻克这部分了。

1. 背景问题：LLM 推理的瓶颈:
    * 复习点: 为什么 LLM 推理慢？关键在于 KV Cache。每生成一个新 token，都需要将这个新 token 的 Key 和 Value 向量与之前所有 token 的 KV 
      向量拼接起来，再进行 Attention 计算。这个 KV Cache 会变得非常大，且长度动态变化，管理困难。
2. vLLM 的解决方案：PagedAttention:
    * 核心思想: 像操作系统的虚拟内存分页一样来管理 KV Cache。
    * `Block`: 将 GPU 显存预先划分为很多个固定大小的块 (Block)。
    * `Block Manager`: 一个管理器 (nanovllm/engine/block_manager.py)，负责分配和释放这些 Block。
    * 逻辑块 vs 物理块: 一个序列的 KV Cache 在逻辑上是连续的，但在物理显存中，它被存储在多个不一定连续的 Block 中。
    * `Block Table`: 每个序列都有一个 "块表"，记录了它的 KV Cache 存储在哪些物理 Block 中。这就像进程的页表。
    * PagedAttention Kernel: 在执行 Attention 计算时，GPU Kernel (CUDA 核心) 不再需要操作连续的大块内存，而是根据 Block Table，直接去各个物理 Block 
      中抓取所需的 Key 和 Value。这极大地提高了显存利用率和吞吐量。
3. `nano-vllm` 中的核心组件:
    * `Sequence` / `SequenceGroup`: (sequence.py) 用于表示一个或一组正在处理的请求（例如 beam search 会有多个序列）。
    * `Scheduler`: (scheduler.py) 调度器，决定当前批次要处理哪些序列，并为它们向 BlockManager 申请 KV Cache 空间。
    * `LLMEngine`: (llm_engine.py) 整个推理引擎的“大脑”，协调 Scheduler, BlockManager 和 ModelRunner。
    * `ModelRunner`: (model_runner.py) 负责执行模型的前向传播。

建议的阅读顺序

1. 从 `example.py` 开始:
    * 通读 example.py，了解一个完整的推理流程是如何被调用的。注意 LLM 类的初始化和 llm.generate() 的调用。
2. 深入 `LLM` 类:
    * 进入 nanovllm/llm.py，查看 LLM 类的实现。它会引导你到 LLMEngine。
3. 探索模型加载:
    * 在 LLMEngine 的初始化中，你会看到模型加载的过程。这时可以跳转到 nanovllm/utils/loader.py 和 
      nanovllm/models/qwen3.py，结合阶段一和二的知识，理解模型是如何被构建和加载权重的。
4. 攻克核心引擎 `nanovllm/engine`:
    * 这是最关键的一步。按照 LLMEngine -> Scheduler -> BlockManager 的顺序，结合上面阶段三中 PagedAttention 的思想，逐一理解每个组件的职责。
    * 重点关注数据结构：Sequence 对象里存了什么？BlockManager 是如何记录可用 Block 的？Scheduler 是如何做出决策的？